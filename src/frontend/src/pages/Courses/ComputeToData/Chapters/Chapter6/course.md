# Chapter 6: Use Cases & Vision

#### Difficulty: **3/5** \| Estimated reading time: **10 min**

<dialog character="whale">“Just like with any deep research, it takes quite a bit of mental agility, and time, to harness the possibilities unlocked by new discoveries.”</dialog>

Nowadays many AI models are open source and research thrives from sharing algorithms in between companies and research facilities. Compute-to-Data is unlocking this, and practical applications span across many different fields.

Consider cancer research. AI could provide break-through discoveries, but the data is lacking to build solid models, because it sits across a multitude of local medical centers, and is highly sensitive. With CtD, it is possible to pool data across a multitude of local medical centers, with the data never leaving their own premises.

Established players are harnessing the new capability first to gain a competitive advantage in their industry, from health, energy, to freight or automotive. But CtD also enables new possibilities entirely; let’s have a look.

**Private AI**

Private AI has become more of a topic as [Openmined](https://www.openmined.org/) has started to heavily advocate and educate the world about it. Their [Udacity course](https://www.udacity.com/course/secure-and-private-ai--ud185) as well as their [own course series](https://courses.openmined.org/) are highly recommended for people interested in the current frontier of private AI.

One of the core conclusions of the course is that “information must not be shared to an extent that reveals too much private information contained in the data”.

With Compute-to-Data only the algorithms actually interact with the data and the algorithm selection is in the hands of the data seller. Algorithms can be audited to prevent data leaks; in fact, public as well as private algorithms sold on Ocean Protocol are testable by everyone.

**Data and Algorithms Auditing**

With Compute-to-Data, Data Owners will need to ensure that algorithms can be trusted, and would welcome a vetting process on algorithms.

Conversely, AI researchers may want to get a signal on dataset quality (and trustful execution of Compute jobs), so data auditing would greatly help them select datasets on which to train their models.

This opens a completely new field of data science: data auditing. Auditors generate information for Data Owners and Consumers without having to risk any data at all (dataset or algorithms).

**Blockchain-powered Federated Learning**

Federated learning was first published by [Google in 2017](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html) and was first used to train auto completion models on individual smartphones. This allows a model to be trained with the data that a user produces while they are using the smartphone without having to send the data to Google. This has been picked up by several other companies e.g. Apple is also using it to [adapt models for language understanding to users](https://arxiv.org/pdf/2102.08503.pdf) in their homes instead of training models in the cloud - with intention to preserve privacy.

Compute-to-Data makes it theoretically possible to do federated learning on any kind of data as the model can be sent to the data to be trained there e.g. via transfer learning and the improved model can then be sent to the next piece of data to be improved there.

This reduces the need for bandwidth in the case of “big” datasets, and it respects privacy in the case of personal “small” data. A lot of data is too big to be transferred to algorithms or the transfer could make it not worth it to use the data.

**Remote aggregation of obfuscated data**

Aggregation of relevant information out of the remote data lakes is another promising new field enabled by CtD. Data in data lakes is not available to download but insights can be generated indirectly by applying algorithms to probe and investigate the datasets in a privacy-preserving way.

This is an iterative process where the analyst or data scientist has to gain insights into the data without actually seeing it, and without revealing anything proprietary. As the amount of data available will grow exponentially the need to aggregate the relevant data across different data lakes is gaining in importance.

These new tools, as well as the aggregated data, are valuable assets that can be sold to other ecosystem participants.

**Data stewardship**

Data Owners may wish to outsource the technical aspects of the Compute-to-Data infrastructure as well as the monetization of the data. Thanks to CtD, handing over the responsibility over the data for a share of the rewards is relatively safe because everything is tracked and logged on the blockchain.

This enables the creation of new datasets composed of other datasets, all accessed via Compute-to-Data. Data composability can be a major driver of more powerful AI models and completely new business models.

For example, car manufacturers can pool their data together to train more powerful autonomous driving AI, while never sending any of their own company driving data anywhere.  Pooling data allows to capture more scenarios and edge cases, limiting bias and error in AI models, increasing compliance and meeting legal requirements for technology classified as critical.

**Data unions**

Data unions are another form of data stewardship for regular people with the aim of pooling data together, monetizing and governing the Data Asset together. Browser history, image gallery or location data of a single person is not very relevant for machine learning but the power is in the numbers - a million users can generate a relevant amount of data quickly.

A data union can earn enough revenue to cover the developing cleaning, aggregation, annotation, and verification services on top of the data and reward users for their data on top. A data union may also train models on the data and sell them.

The pooling of more and more diverse inputs into datasets for AI will yield the most powerful machine learning models in human history as their dataset and annotations exceed the amount and quality of all other datasets that ever existed ([see the long tail problem](https://a16z.com/2020/07/24/long-tail-problem-in-a-i/) described by a16z, a leading VC fund).

For the first time in human history, scientists will be able to systematically remove bias from all data and models, by including all of humanity across nationalities, ethnicities, genders, age groups (etc.) in the training Data Sets.

**What next?**

Unlike scarce material resources, information and knowledge (data) are assets that have increasing returns, i.e. the more people access, use and modify data, the greater its value becomes. Once network effects kick in, the value of data can go exponential.

Societies as a whole will benefit from the global decentralized data network in ways that could not be imagined before, because people now have ownership over their private data, and can participate and be rewarded in the creation of value.

With more data, investments into the critical parts of the data economy’s infrastructure and education could be allocated in an efficient way, based on empirical evidence obtained from the large quantities of new available data.
