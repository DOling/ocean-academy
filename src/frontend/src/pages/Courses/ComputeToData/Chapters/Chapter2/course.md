# Chapter 2: The Ocean Protocol Compute Layer

#### Difficulty: **2/5** \| Estimated reading time: **5 min**

<dialog character="shell">“Because mating in the dark doesn’t expose nudity, but it bears the same offspring...”</dialog>


**Compute-to-Data. **Compute-to-Data is a technology solution that allows algorithms to work on data while staying in control of data at all times. Figuratively, it forms a protective layer between the data and the person consuming the data. It is a paradigm shift to shift the default towards data sovereignty.

Data Owners can earn revenue from AI and data scientists use the data without leaking any information on the data itself as the data never leaves its premises; Compute-to-Data enables privacy-preserving data monetization. This encourages greater value creation, and in turn, more data sharing.

On the other side, Data Consumers can run remote Compute jobs on data that historically would not have been previously available due to risks of data breaches.

<img src="/images/CtD/chapter_2_0.png" />

Data Owners provide Compute power on their premises and will run algorithms from Data Consumers locally, only returning results, not the data itself.

Secure algorithms ensure that the computed results returned are sufficiently aggregated and anonymized so that the data is fully obfuscated and the privacy risk is minimized.

This solution is safe, but it does require Data Owners to do two things:

1. Provide data computing capabilities and access control on the data, and
2. Carefully select what algorithms to run on the data, based on how safe they are.

Of course, either point could be delegated to a trusted third party, but this is as safe as the third party is, and it entails risks to be aware of.

The third party could be the single point of failure in this setup, just like privacy-preserving data marketplaces described in chapter 1.

**Ocean Protocol eliminates the drawbacks of traditional data marketplaces by cutting out the intermediary**.

Consensus on access and operations on data is handled in a decentralized and trustless way using smart contracts rather than an intermediary. If you are not yet familiar with Ocean Protocol and blockchain yet, you will get all this context in the [Ocean 101 course](https://oceanacademy.io/ocean101/chapter-1).

The private computing layer introduced by CtD offers numerous advantages over legacy data sharing methods, effectively addressing the most common barriers to collaboration on data and AI.

**Benefits of CtD to Data Owners:**

* **Asset safety. **Data Assets cannot be copied and re-used from the approved Compute job.
* **Privacy.** Data Consumers do not receive a copy of the data. They receive the aggregated and/or anonymized results so they cannot leak personal or sensitive information.
* **Control.** Data Owners retain full control of their data, as the data is never moved outside of their premises.
* **Efficiency in data processing**. Data Owners do not need to anonymize their data for it to be used by algorithms externally.
* **New revenue stream**. CtD enables Data Owners to monetize data that was previously only used internally due to safety concerns.

**Benefits of CtD to Data Consumers:**

* **Extra data.** By incentivizing data sharing, the Compute-to-Data layer will make more data available to data products builders.
* **No need for computation infrastructure**. Computation is handled by Data Owners, so Data Consumers do not need any equipment.
* **Data legos**. Data can be safely combined with other data and algorithms, for aggregation or validation.

**Benefits of CtD to both Data Owners and Data Consumers:**

* **Storage capacity.** Data Owners can sell Compute access without having to move any data (e.g. to the Data Consumers), which is ideal for very large datasets that are slow or expensive to move.
* **Transfer capacity.** In some cases, like in the race to create autonomous driving cars, data gets created faster than bandwidth can move, so the only way to create value from data is on-premise.
* **Compliance.** Having only one copy of the data and not moving it makes it easier to comply with data protection regulations. For instance, Compute-to-Data follows the European data protection regulation principle of purpose-limitation because the data is never moved anywhere.
* **No single point of failure. **There does not exist a central entity that can shut your business down. Access control and market functionality is managed by Smart Contracts.
* **Auditability**. Compute-to-Data gives proof that algorithms were properly executed, on which data they were executed and it enables robust and audit-proof data collection plans. This way anyone can re-use the exact same data and benchmark against other algorithms or data.
* **Explainability.** Compute-to-Data contributes to AI explainability, because it creates an immutable audit trail of AI training.
* **Accuracy improvements**. Computation results from different datasets and algorithms can be compared, based on the audit trail they leave on the blockchain. This leads to competition among products, innovation and excellence under free market conditions which benefits everyone.

Yes, just that.
